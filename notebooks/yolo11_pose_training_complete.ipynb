{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6817ab",
   "metadata": {},
   "source": [
    "# YOLO11 Pose Estimation for Waste Throwing Detection\n",
    "\n",
    "This comprehensive notebook guides you through training a YOLO11 pose estimation model to detect people throwing waste from video data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Install Required Libraries](#install)\n",
    "2. [Import Libraries and Setup](#setup)\n",
    "3. [Prepare Video Dataset](#prepare)\n",
    "4. [Extract Frames from Videos](#extract)\n",
    "5. [Annotate Data for Pose Estimation](#annotate)\n",
    "6. [Convert Annotations to YOLO Format](#convert)\n",
    "7. [Setup YOLO11 Pose Model](#model)\n",
    "8. [Configure Training Parameters](#config)\n",
    "9. [Train the Model](#train)\n",
    "10. [Validate Model Performance](#validate)\n",
    "11. [Test on New Video Data](#test)\n",
    "12. [Visualize Pose Detection Results](#visualize)\n",
    "\n",
    "## Project Overview\n",
    "- **Goal**: Detect people throwing waste using pose estimation\n",
    "- **Model**: YOLO11 with pose estimation capabilities\n",
    "- **Input**: Videos of people throwing waste\n",
    "- **Output**: Trained model that can detect throwing poses in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340f1ab",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries and Dependencies {#install}\n",
    "\n",
    "First, let's install all the necessary libraries for our pose estimation project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a3bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell only once when setting up the environment\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Core packages for YOLO11 and pose estimation\n",
    "packages = [\n",
    "    \"ultralytics>=8.0.0\",  # YOLO11 framework\n",
    "    \"opencv-python>=4.7.0\",  # Computer vision\n",
    "    \"torch>=1.13.0\",  # PyTorch\n",
    "    \"torchvision>=0.14.0\",  # PyTorch vision\n",
    "    \"numpy>=1.21.0\",  # Numerical computing\n",
    "    \"matplotlib>=3.5.0\",  # Plotting\n",
    "    \"seaborn>=0.11.0\",  # Statistical visualization\n",
    "    \"pandas>=1.4.0\",  # Data manipulation\n",
    "    \"Pillow>=9.0.0\",  # Image processing\n",
    "    \"tqdm>=4.64.0\",  # Progress bars\n",
    "    \"PyYAML>=6.0\",  # YAML configuration\n",
    "    \"scikit-learn>=1.1.0\",  # Machine learning utilities\n",
    "    \"moviepy>=1.0.0\",  # Video processing\n",
    "    \"wandb>=0.13.0\",  # Experiment tracking (optional)\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        install_package(package)\n",
    "        print(f\"‚úÖ Successfully installed {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Installation complete! Please restart the kernel after installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b435ea",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup Environment {#setup}\n",
    "\n",
    "Now let's import all necessary libraries and set up our working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518210ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Import YOLO11 and PyTorch\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Import scikit-learn for data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"üñ•Ô∏è  System Information:\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Create project directory structure\n",
    "project_root = Path(\"../\")  # Assuming notebook is in notebooks/ folder\n",
    "data_dir = project_root / \"data\"\n",
    "videos_dir = data_dir / \"videos\"\n",
    "frames_dir = data_dir / \"frames\"\n",
    "annotations_dir = data_dir / \"annotations\"\n",
    "models_dir = project_root / \"models\"\n",
    "results_dir = project_root / \"results\"\n",
    "\n",
    "# Create directories\n",
    "for directory in [videos_dir, frames_dir, annotations_dir, models_dir, results_dir]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nüìÅ Project Structure Created:\")\n",
    "print(f\"Videos: {videos_dir}\")\n",
    "print(f\"Frames: {frames_dir}\")\n",
    "print(f\"Annotations: {annotations_dir}\")\n",
    "print(f\"Models: {models_dir}\")\n",
    "print(f\"Results: {results_dir}\")\n",
    "\n",
    "# Define YOLO11 pose keypoints (COCO format - 17 keypoints)\n",
    "KEYPOINT_NAMES = [\n",
    "    'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
    "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle'\n",
    "]\n",
    "\n",
    "# Key keypoints for throwing detection\n",
    "THROWING_KEYPOINTS = [\n",
    "    'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
    "    'left_wrist', 'right_wrist', 'left_hip', 'right_hip'\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ Pose keypoints defined: {len(KEYPOINT_NAMES)} total keypoints\")\n",
    "print(f\"üéØ Throwing-specific keypoints: {len(THROWING_KEYPOINTS)} keypoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126b0d8",
   "metadata": {},
   "source": [
    "## 3. Prepare Video Dataset {#prepare}\n",
    "\n",
    "In this section, we'll prepare your video dataset for training. Place your videos of people throwing waste in the `data/videos/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze video dataset\n",
    "def analyze_video_dataset(videos_directory):\n",
    "    \"\"\"Analyze the video dataset and provide statistics\"\"\"\n",
    "    \n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv']\n",
    "    video_files = []\n",
    "    \n",
    "    # Find all video files\n",
    "    for ext in video_extensions:\n",
    "        video_files.extend(list(videos_directory.glob(f'*{ext}')))\n",
    "        video_files.extend(list(videos_directory.glob(f'*{ext.upper()}')))\n",
    "    \n",
    "    if not video_files:\n",
    "        print(\"‚ùå No video files found!\")\n",
    "        print(f\"üìÅ Please place your videos in: {videos_directory}\")\n",
    "        print(f\"üìπ Supported formats: {', '.join(video_extensions)}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìπ Found {len(video_files)} video files\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_duration = 0\n",
    "    video_info = []\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        try:\n",
    "            # Open video and get properties\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            \n",
    "            if not cap.isOpened():\n",
    "                print(f\"‚ùå Cannot open: {video_path.name}\")\n",
    "                continue\n",
    "            \n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            duration = frame_count / fps if fps > 0 else 0\n",
    "            \n",
    "            total_duration += duration\n",
    "            \n",
    "            video_info.append({\n",
    "                'filename': video_path.name,\n",
    "                'duration': duration,\n",
    "                'fps': fps,\n",
    "                'frames': frame_count,\n",
    "                'resolution': f\"{width}x{height}\",\n",
    "                'size_mb': video_path.stat().st_size / (1024 * 1024)\n",
    "            })\n",
    "            \n",
    "            print(f\"üìÑ {video_path.name}\")\n",
    "            print(f\"   ‚è±Ô∏è  Duration: {duration:.1f} seconds\")\n",
    "            print(f\"   üé¨ FPS: {fps:.1f}\")\n",
    "            print(f\"   üñºÔ∏è  Resolution: {width}x{height}\")\n",
    "            print(f\"   üìä Frames: {frame_count}\")\n",
    "            print(f\"   üíæ Size: {video_path.stat().st_size / (1024 * 1024):.1f} MB\")\n",
    "            print()\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {video_path.name}: {e}\")\n",
    "    \n",
    "    print(\"üìä Dataset Summary:\")\n",
    "    print(f\"   üé¨ Total videos: {len(video_info)}\")\n",
    "    print(f\"   ‚è±Ô∏è  Total duration: {total_duration:.1f} seconds ({total_duration/60:.1f} minutes)\")\n",
    "    print(f\"   üìä Average duration: {total_duration/len(video_info):.1f} seconds\")\n",
    "    print(f\"   üíæ Total size: {sum(info['size_mb'] for info in video_info):.1f} MB\")\n",
    "    \n",
    "    return video_info\n",
    "\n",
    "# Analyze the video dataset\n",
    "print(\"üîç Analyzing your video dataset...\")\n",
    "video_info = analyze_video_dataset(videos_dir)\n",
    "\n",
    "# Recommendations based on dataset\n",
    "if video_info:\n",
    "    total_duration = sum(info['duration'] for info in video_info)\n",
    "    \n",
    "    print(\"\\nüí° Recommendations:\")\n",
    "    if total_duration < 300:  # Less than 5 minutes\n",
    "        print(\"‚ö†Ô∏è  Small dataset detected (< 5 minutes total)\")\n",
    "        print(\"   - Consider adding more videos for better training\")\n",
    "        print(\"   - Use data augmentation strategies\")\n",
    "        print(\"   - Extract frames at higher frequency\")\n",
    "    elif total_duration < 1800:  # Less than 30 minutes\n",
    "        print(\"‚úÖ Moderate dataset size (5-30 minutes)\")\n",
    "        print(\"   - Good for initial training\")\n",
    "        print(\"   - Consider fine-tuning strategies\")\n",
    "    else:\n",
    "        print(\"‚úÖ Large dataset detected (> 30 minutes)\")\n",
    "        print(\"   - Excellent for robust training\")\n",
    "        print(\"   - Can use standard training procedures\")\n",
    "else:\n",
    "    print(\"\\nüìù Next Steps:\")\n",
    "    print(\"1. Add your video files to the data/videos/ directory\")\n",
    "    print(\"2. Supported formats: .mp4, .avi, .mov, .mkv, .wmv, .flv\")\n",
    "    print(\"3. Re-run this cell to analyze your dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943b4fb",
   "metadata": {},
   "source": [
    "## 4. Extract Frames from Videos {#extract}\n",
    "\n",
    "Now we'll extract frames from your videos to create training data. We'll use intelligent extraction strategies to get the best frames for annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd443955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameExtractor:\n",
    "    \"\"\"Advanced frame extraction with multiple strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir, strategy='uniform', fps_target=2):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.strategy = strategy\n",
    "        self.fps_target = fps_target\n",
    "        self.metadata = []\n",
    "        \n",
    "    def extract_uniform(self, video_path, video_name):\n",
    "        \"\"\"Extract frames at uniform intervals\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_interval = max(1, int(fps / self.fps_target))\n",
    "        \n",
    "        video_output_dir = self.output_dir / video_name\n",
    "        video_output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        frame_count = 0\n",
    "        extracted_count = 0\n",
    "        \n",
    "        pbar = tqdm(desc=f\"Extracting from {video_name}\")\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if frame_count % frame_interval == 0:\n",
    "                frame_filename = f\"{video_name}_frame_{extracted_count:06d}.jpg\"\n",
    "                frame_path = video_output_dir / frame_filename\n",
    "                \n",
    "                cv2.imwrite(str(frame_path), frame)\n",
    "                \n",
    "                self.metadata.append({\n",
    "                    'video_name': video_name,\n",
    "                    'frame_number': frame_count,\n",
    "                    'extracted_frame_id': extracted_count,\n",
    "                    'timestamp': frame_count / fps,\n",
    "                    'frame_path': str(frame_path)\n",
    "                })\n",
    "                \n",
    "                extracted_count += 1\n",
    "            \n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "        cap.release()\n",
    "        pbar.close()\n",
    "        return extracted_count\n",
    "    \n",
    "    def extract_motion_based(self, video_path, video_name):\n",
    "        \"\"\"Extract frames based on motion detection\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        ret, prev_frame = cap.read()\n",
    "        if not ret:\n",
    "            return 0\n",
    "            \n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        video_output_dir = self.output_dir / video_name\n",
    "        video_output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        frame_count = 0\n",
    "        extracted_count = 0\n",
    "        motion_threshold = 15000  # Adjust based on needs\n",
    "        \n",
    "        pbar = tqdm(desc=f\"Motion-based extraction from {video_name}\")\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            diff = cv2.absdiff(prev_gray, gray)\n",
    "            motion_score = np.sum(diff)\n",
    "            \n",
    "            if motion_score > motion_threshold:\n",
    "                frame_filename = f\"{video_name}_motion_{extracted_count:06d}.jpg\"\n",
    "                frame_path = video_output_dir / frame_filename\n",
    "                \n",
    "                cv2.imwrite(str(frame_path), frame)\n",
    "                \n",
    "                self.metadata.append({\n",
    "                    'video_name': video_name,\n",
    "                    'frame_number': frame_count,\n",
    "                    'extracted_frame_id': extracted_count,\n",
    "                    'timestamp': frame_count / fps,\n",
    "                    'motion_score': float(motion_score),\n",
    "                    'frame_path': str(frame_path)\n",
    "                })\n",
    "                \n",
    "                extracted_count += 1\n",
    "                prev_gray = gray\n",
    "            \n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "        cap.release()\n",
    "        pbar.close()\n",
    "        return extracted_count\n",
    "    \n",
    "    def process_videos(self, videos_directory):\n",
    "        \"\"\"Process all videos in directory\"\"\"\n",
    "        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv']\n",
    "        video_files = []\n",
    "        \n",
    "        for ext in video_extensions:\n",
    "            video_files.extend(list(videos_directory.glob(f'*{ext}')))\n",
    "            video_files.extend(list(videos_directory.glob(f'*{ext.upper()}')))\n",
    "        \n",
    "        if not video_files:\n",
    "            print(\"‚ùå No video files found!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üé¨ Processing {len(video_files)} videos...\")\n",
    "        print(f\"üìä Strategy: {self.strategy}\")\n",
    "        print(f\"üéØ Target FPS: {self.fps_target}\")\n",
    "        \n",
    "        total_extracted = 0\n",
    "        \n",
    "        for video_path in video_files:\n",
    "            video_name = video_path.stem\n",
    "            print(f\"\\nüìπ Processing: {video_name}\")\n",
    "            \n",
    "            if self.strategy == 'uniform':\n",
    "                extracted = self.extract_uniform(video_path, video_name)\n",
    "            elif self.strategy == 'motion':\n",
    "                extracted = self.extract_motion_based(video_path, video_name)\n",
    "            else:\n",
    "                print(f\"Unknown strategy: {self.strategy}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"‚úÖ Extracted {extracted} frames\")\n",
    "            total_extracted += extracted\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = self.output_dir / 'extraction_metadata.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüéâ Extraction Complete!\")\n",
    "        print(f\"üìä Total frames extracted: {total_extracted}\")\n",
    "        print(f\"üìÑ Metadata saved: {metadata_path}\")\n",
    "        \n",
    "        return total_extracted\n",
    "\n",
    "# Configure frame extraction\n",
    "EXTRACTION_STRATEGY = 'uniform'  # Options: 'uniform', 'motion'\n",
    "TARGET_FPS = 2  # Frames per second to extract\n",
    "\n",
    "print(\"üé¨ Frame Extraction Configuration:\")\n",
    "print(f\"Strategy: {EXTRACTION_STRATEGY}\")\n",
    "print(f\"Target FPS: {TARGET_FPS}\")\n",
    "\n",
    "# Create frame extractor\n",
    "extractor = FrameExtractor(\n",
    "    output_dir=frames_dir,\n",
    "    strategy=EXTRACTION_STRATEGY,\n",
    "    fps_target=TARGET_FPS\n",
    ")\n",
    "\n",
    "# Extract frames from videos\n",
    "if video_info:  # Only if videos were found\n",
    "    total_frames = extractor.process_videos(videos_dir)\n",
    "    \n",
    "    if total_frames > 0:\n",
    "        print(f\"\\nüí° Next Steps:\")\n",
    "        print(f\"1. Review extracted frames in: {frames_dir}\")\n",
    "        print(f\"2. Proceed to annotation step\")\n",
    "        print(f\"3. For best results, aim for 500-1000+ annotated frames\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No videos found. Please add videos to data/videos/ directory first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a841c4d",
   "metadata": {},
   "source": [
    "## 5. Annotate Data for Pose Estimation {#annotate}\n",
    "\n",
    "This is a critical step where you'll annotate the keypoints for human poses. Since this requires manual annotation, we'll provide guidance and tools to make the process efficient.\n",
    "\n",
    "### Annotation Tools Options:\n",
    "\n",
    "1. **CVAT (Computer Vision Annotation Tool)** - Recommended\n",
    "   - Web-based annotation platform\n",
    "   - Supports pose keypoint annotation\n",
    "   - Team collaboration features\n",
    "   \n",
    "2. **LabelImg** - For bounding boxes (if needed)\n",
    "   - Simple desktop application\n",
    "   - Good for object detection\n",
    "\n",
    "3. **Roboflow** - Online annotation platform\n",
    "   - User-friendly interface\n",
    "   - Built-in data management\n",
    "\n",
    "### YOLO11 Pose Annotation Format:\n",
    "\n",
    "Each annotation file should contain:\n",
    "```\n",
    "class_id x_center y_center width height x1 y1 v1 x2 y2 v2 ... x17 y17 v17\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `class_id`: Object class (0 for person)\n",
    "- `x_center, y_center, width, height`: Bounding box (normalized 0-1)\n",
    "- `xi, yi, vi`: Keypoint coordinates (normalized) and visibility\n",
    "  - `vi = 0`: Not visible\n",
    "  - `vi = 1`: Visible\n",
    "  - `vi = 2`: Occluded but labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b052bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation Helper Functions\n",
    "\n",
    "def create_sample_annotation():\n",
    "    \"\"\"Create a sample annotation file for reference\"\"\"\n",
    "    sample_annotation = \"\"\"# Sample YOLO11 Pose Annotation Format\n",
    "# Each line represents one person in the image\n",
    "# Format: class_id x_center y_center width height x1 y1 v1 x2 y2 v2 ... x17 y17 v17\n",
    "\n",
    "# Example annotation for a person:\n",
    "# 0 0.5 0.6 0.3 0.8 0.52 0.32 2 0.48 0.31 2 0.56 0.31 2 0.45 0.33 1 0.59 0.33 1 0.42 0.45 2 0.58 0.45 2 0.38 0.55 2 0.62 0.55 2 0.35 0.58 2 0.65 0.58 2 0.47 0.78 2 0.53 0.78 2 0.45 0.95 2 0.55 0.95 2 0.43 1.0 1 0.57 1.0 1\n",
    "\n",
    "# Keypoint order (17 keypoints):\n",
    "# 0: nose\n",
    "# 1: left_eye, 2: right_eye\n",
    "# 3: left_ear, 4: right_ear  \n",
    "# 5: left_shoulder, 6: right_shoulder\n",
    "# 7: left_elbow, 8: right_elbow\n",
    "# 9: left_wrist, 10: right_wrist\n",
    "# 11: left_hip, 12: right_hip\n",
    "# 13: left_knee, 14: right_knee\n",
    "# 15: left_ankle, 16: right_ankle\n",
    "\n",
    "# For throwing detection, focus on:\n",
    "# - Shoulders (5, 6): Position indicates body orientation\n",
    "# - Elbows (7, 8): Arm bending during throw\n",
    "# - Wrists (9, 10): Hand position and motion direction\n",
    "# - Hips (11, 12): Lower body stability during throw\n",
    "\"\"\"\n",
    "    \n",
    "    sample_path = annotations_dir / 'annotation_format_reference.txt'\n",
    "    with open(sample_path, 'w') as f:\n",
    "        f.write(sample_annotation)\n",
    "    \n",
    "    print(f\"üìù Sample annotation saved to: {sample_path}\")\n",
    "\n",
    "def validate_annotation_file(annotation_path):\n",
    "    \"\"\"Validate a single annotation file\"\"\"\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        valid_lines = 0\n",
    "        errors = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "                \n",
    "            parts = line.split()\n",
    "            \n",
    "            # Check format: 1 class + 4 bbox + 51 keypoints (17 * 3)\n",
    "            if len(parts) != 56:\n",
    "                errors.append(f\"Line {i+1}: Expected 56 values, got {len(parts)}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Validate class_id\n",
    "                class_id = int(parts[0])\n",
    "                if class_id != 0:\n",
    "                    errors.append(f\"Line {i+1}: Class ID should be 0 for person, got {class_id}\")\n",
    "                \n",
    "                # Validate bbox (normalized 0-1)\n",
    "                bbox = list(map(float, parts[1:5]))\n",
    "                if not all(0 <= val <= 1 for val in bbox):\n",
    "                    errors.append(f\"Line {i+1}: Bbox values should be normalized (0-1)\")\n",
    "                \n",
    "                # Validate keypoints\n",
    "                keypoints = list(map(float, parts[5:]))\n",
    "                for j in range(0, len(keypoints), 3):\n",
    "                    x, y, v = keypoints[j], keypoints[j+1], keypoints[j+2]\n",
    "                    if not (0 <= x <= 1 and 0 <= y <= 1 and v in [0, 1, 2]):\n",
    "                        errors.append(f\"Line {i+1}: Invalid keypoint {j//3 + 1}\")\n",
    "                        break\n",
    "                \n",
    "                valid_lines += 1\n",
    "                \n",
    "            except ValueError as e:\n",
    "                errors.append(f\"Line {i+1}: Value error - {e}\")\n",
    "        \n",
    "        return valid_lines, errors\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0, [f\"File error: {e}\"]\n",
    "\n",
    "def check_annotation_status():\n",
    "    \"\"\"Check the status of annotation files\"\"\"\n",
    "    \n",
    "    # Look for annotation files\n",
    "    annotation_files = list(annotations_dir.glob('*.txt'))\n",
    "    annotation_files = [f for f in annotation_files if not f.name.startswith('annotation_format')]\n",
    "    \n",
    "    if not annotation_files:\n",
    "        print(\"‚ùå No annotation files found!\")\n",
    "        print(f\"üìÅ Expected location: {annotations_dir}\")\n",
    "        print(\"\\nüìã To create annotations:\")\n",
    "        print(\"1. Use CVAT, LabelImg, or similar tool\")\n",
    "        print(\"2. Export in YOLO format\")\n",
    "        print(\"3. Place .txt files in annotations/ directory\")\n",
    "        print(\"4. Each .txt file should match a frame filename\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üìä Found {len(annotation_files)} annotation files\")\n",
    "    \n",
    "    total_valid = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    print(\"\\nüîç Validating annotations...\")\n",
    "    for ann_file in tqdm(annotation_files[:10]):  # Check first 10 files\n",
    "        valid_lines, errors = validate_annotation_file(ann_file)\n",
    "        total_valid += valid_lines\n",
    "        total_errors += len(errors)\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"‚ö†Ô∏è  {ann_file.name}: {len(errors)} errors\")\n",
    "            for error in errors[:3]:  # Show first 3 errors\n",
    "                print(f\"   {error}\")\n",
    "            if len(errors) > 3:\n",
    "                print(f\"   ... and {len(errors)-3} more errors\")\n",
    "    \n",
    "    print(f\"\\nüìà Validation Results:\")\n",
    "    print(f\"   ‚úÖ Valid annotations: {total_valid}\")\n",
    "    print(f\"   ‚ùå Total errors: {total_errors}\")\n",
    "    \n",
    "    if total_valid > 0:\n",
    "        print(\"‚úÖ Ready for training preparation!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Please fix annotation errors before proceeding\")\n",
    "        return False\n",
    "\n",
    "# Create reference files\n",
    "create_sample_annotation()\n",
    "\n",
    "# Check annotation status\n",
    "print(\"üîç Checking annotation status...\")\n",
    "annotations_ready = check_annotation_status()\n",
    "\n",
    "if not annotations_ready:\n",
    "    print(\"\\nüìã Annotation Checklist:\")\n",
    "    print(\"‚ñ° Extract frames from videos (completed above)\")\n",
    "    print(\"‚ñ° Select representative frames for annotation (~200-500 minimum)\")\n",
    "    print(\"‚ñ° Use CVAT or similar tool to annotate keypoints\")\n",
    "    print(\"‚ñ° Export annotations in YOLO pose format\")\n",
    "    print(\"‚ñ° Place annotation files in data/annotations/\")\n",
    "    print(\"‚ñ° Validate annotations (run this cell again)\")\n",
    "    print(\"‚ñ° Proceed to training\")\n",
    "    \n",
    "    print(\"\\nüîó Useful Resources:\")\n",
    "    print(\"- CVAT: https://cvat.org\")\n",
    "    print(\"- YOLO format guide: https://docs.ultralytics.com/datasets/pose/\")\n",
    "    print(\"- Pose annotation tutorial: https://blog.roboflow.com/pose-estimation-annotation/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e428952",
   "metadata": {},
   "source": [
    "## 6. Convert Annotations to YOLO Format {#convert}\n",
    "\n",
    "Now we'll organize our annotated data into the proper YOLO11 training format with train/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbefc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_yolo_dataset(train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    \"\"\"Prepare YOLO format dataset with train/val/test splits\"\"\"\n",
    "    \n",
    "    print(\"üì¶ Preparing YOLO dataset...\")\n",
    "    \n",
    "    # Find all annotated image-annotation pairs\n",
    "    frame_files = []\n",
    "    for ext in ['.jpg', '.jpeg', '.png']:\n",
    "        frame_files.extend(list(frames_dir.glob(f'**/*{ext}')))\n",
    "        frame_files.extend(list(frames_dir.glob(f'**/*{ext.upper()}')))\n",
    "    \n",
    "    # Find corresponding annotations\n",
    "    pairs = []\n",
    "    missing_annotations = []\n",
    "    \n",
    "    for img_path in frame_files:\n",
    "        ann_name = img_path.stem + '.txt'\n",
    "        ann_path = annotations_dir / ann_name\n",
    "        \n",
    "        if ann_path.exists():\n",
    "            pairs.append((img_path, ann_path))\n",
    "        else:\n",
    "            missing_annotations.append(img_path)\n",
    "    \n",
    "    print(f\"üìä Found {len(pairs)} image-annotation pairs\")\n",
    "    if missing_annotations:\n",
    "        print(f\"‚ö†Ô∏è  {len(missing_annotations)} images without annotations\")\n",
    "    \n",
    "    if len(pairs) < 10:\n",
    "        print(\"‚ùå Insufficient annotated data! Need at least 10 pairs for training.\")\n",
    "        return False\n",
    "    \n",
    "    # Split dataset\n",
    "    print(f\"üìà Splitting dataset: {train_ratio:.0%} train, {val_ratio:.0%} val, {test_ratio:.0%} test\")\n",
    "    \n",
    "    # First split: separate test set\n",
    "    train_val_pairs, test_pairs = train_test_split(pairs, test_size=test_ratio, random_state=42)\n",
    "    \n",
    "    # Second split: separate train and validation  \n",
    "    val_size = val_ratio / (train_ratio + val_ratio)\n",
    "    train_pairs, val_pairs = train_test_split(train_val_pairs, test_size=val_size, random_state=42)\n",
    "    \n",
    "    print(f\"‚úÖ Train: {len(train_pairs)} samples\")\n",
    "    print(f\"‚úÖ Validation: {len(val_pairs)} samples\") \n",
    "    print(f\"‚úÖ Test: {len(test_pairs)} samples\")\n",
    "    \n",
    "    # Create directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        (data_dir / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (data_dir / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy files to appropriate directories\n",
    "    def copy_files(pairs_list, split_name):\n",
    "        print(f\"üìÅ Copying {split_name} files...\")\n",
    "        for img_path, ann_path in tqdm(pairs_list):\n",
    "            # Copy image\n",
    "            img_dest = data_dir / split_name / 'images' / img_path.name\n",
    "            shutil.copy2(img_path, img_dest)\n",
    "            \n",
    "            # Copy annotation\n",
    "            ann_dest = data_dir / split_name / 'labels' / ann_path.name\n",
    "            shutil.copy2(ann_path, ann_dest)\n",
    "    \n",
    "    copy_files(train_pairs, 'train')\n",
    "    copy_files(val_pairs, 'val')\n",
    "    if test_pairs:\n",
    "        copy_files(test_pairs, 'test')\n",
    "    \n",
    "    # Create dataset configuration file\n",
    "    dataset_config = {\n",
    "        'path': str(data_dir.absolute()),\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'test': 'test/images',\n",
    "        'names': {0: 'person'},\n",
    "        'nc': 1,  # number of classes\n",
    "        'kpt_shape': [17, 3],  # 17 keypoints, each with x, y, visibility\n",
    "    }\n",
    "    \n",
    "    config_path = data_dir / 'dataset.yaml'\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset configuration saved: {config_path}\")\n",
    "    \n",
    "    # Create dataset summary\n",
    "    summary = {\n",
    "        'total_samples': len(pairs),\n",
    "        'train_samples': len(train_pairs),\n",
    "        'val_samples': len(val_pairs),\n",
    "        'test_samples': len(test_pairs),\n",
    "        'missing_annotations': len(missing_annotations),\n",
    "        'dataset_ready': True,\n",
    "        'config_path': str(config_path)\n",
    "    }\n",
    "    \n",
    "    summary_path = data_dir / 'dataset_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset summary saved: {summary_path}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def visualize_dataset_sample():\n",
    "    \"\"\"Visualize a sample from the prepared dataset\"\"\"\n",
    "    \n",
    "    train_images_dir = data_dir / 'train' / 'images'\n",
    "    train_labels_dir = data_dir / 'train' / 'labels'\n",
    "    \n",
    "    if not train_images_dir.exists():\n",
    "        print(\"‚ùå Training dataset not prepared yet!\")\n",
    "        return\n",
    "    \n",
    "    # Get a random sample\n",
    "    image_files = list(train_images_dir.glob('*.jpg'))\n",
    "    if not image_files:\n",
    "        image_files = list(train_images_dir.glob('*.png'))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"‚ùå No training images found!\")\n",
    "        return\n",
    "    \n",
    "    sample_img = random.choice(image_files)\n",
    "    sample_label = train_labels_dir / f\"{sample_img.stem}.txt\"\n",
    "    \n",
    "    if not sample_label.exists():\n",
    "        print(f\"‚ùå No label file for {sample_img.name}\")\n",
    "        return\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(str(sample_img))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img_rgb.shape[:2]\n",
    "    \n",
    "    # Load annotations\n",
    "    with open(sample_label, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f'Training Sample: {sample_img.name}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Draw annotations\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 56:\n",
    "            # Extract bbox\n",
    "            x_center, y_center, width, height = map(float, parts[1:5])\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            x_center *= w\n",
    "            y_center *= h\n",
    "            width *= w\n",
    "            height *= h\n",
    "            \n",
    "            # Draw bounding box\n",
    "            x1 = x_center - width/2\n",
    "            y1 = y_center - height/2\n",
    "            x2 = x_center + width/2  \n",
    "            y2 = y_center + height/2\n",
    "            \n",
    "            plt.plot([x1, x2, x2, x1, x1], [y1, y1, y2, y2, y1], 'r-', linewidth=2)\n",
    "            \n",
    "            # Extract and draw keypoints\n",
    "            keypoints = list(map(float, parts[5:]))\n",
    "            for i in range(0, len(keypoints), 3):\n",
    "                if i+2 < len(keypoints):\n",
    "                    x, y, v = keypoints[i], keypoints[i+1], keypoints[i+2]\n",
    "                    if v > 0:  # Visible keypoint\n",
    "                        x_px, y_px = x * w, y * h\n",
    "                        color = 'red' if i//3 in [5, 6, 7, 8, 9, 10] else 'blue'  # Highlight arm keypoints\n",
    "                        plt.scatter(x_px, y_px, c=color, s=30, alpha=0.8)\n",
    "                        \n",
    "                        # Add keypoint labels for key points\n",
    "                        if i//3 in [5, 6, 9, 10]:  # Shoulders and wrists\n",
    "                            kp_name = KEYPOINT_NAMES[i//3] if i//3 < len(KEYPOINT_NAMES) else str(i//3)\n",
    "                            plt.text(x_px, y_px-10, kp_name, fontsize=8, ha='center', color='white',\n",
    "                                   bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üì∑ Sample visualization shown for: {sample_img.name}\")\n",
    "    print(f\"üéØ Red points: Arm/throwing keypoints\")\n",
    "    print(f\"üîµ Blue points: Other body keypoints\")\n",
    "\n",
    "# Prepare the dataset\n",
    "if annotations_ready:\n",
    "    dataset_ready = prepare_yolo_dataset()\n",
    "    \n",
    "    if dataset_ready:\n",
    "        print(\"\\nüéâ Dataset preparation complete!\")\n",
    "        print(\"\\nüìä Dataset Structure:\")\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            split_dir = data_dir / split\n",
    "            if split_dir.exists():\n",
    "                img_count = len(list((split_dir / 'images').glob('*.*')))\n",
    "                label_count = len(list((split_dir / 'labels').glob('*.txt')))\n",
    "                print(f\"   {split}: {img_count} images, {label_count} labels\")\n",
    "        \n",
    "        # Show a sample\n",
    "        print(\"\\nüñºÔ∏è  Visualizing training sample...\")\n",
    "        visualize_dataset_sample()\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Dataset preparation failed!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please complete annotation step first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da3a9c",
   "metadata": {},
   "source": [
    "## 7. Setup YOLO11 Pose Model {#model}\n",
    "\n",
    "Now we'll set up the YOLO11 pose estimation model. YOLO11 comes in different sizes, each with trade-offs between speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO11 Model Configuration\n",
    "\n",
    "# Available model sizes (choose based on your needs)\n",
    "MODEL_SIZES = {\n",
    "    'n': {'name': 'yolo11n-pose.pt', 'description': 'Nano - Fastest, smallest, lower accuracy'},\n",
    "    's': {'name': 'yolo11s-pose.pt', 'description': 'Small - Good balance of speed and accuracy'},\n",
    "    'm': {'name': 'yolo11m-pose.pt', 'description': 'Medium - Better accuracy, moderate speed'},\n",
    "    'l': {'name': 'yolo11l-pose.pt', 'description': 'Large - High accuracy, slower'},\n",
    "    'x': {'name': 'yolo11x-pose.pt', 'description': 'Extra Large - Highest accuracy, slowest'}\n",
    "}\n",
    "\n",
    "def select_model_size():\n",
    "    \"\"\"Help user select appropriate model size\"\"\"\n",
    "    print(\"ü§ñ YOLO11 Pose Model Sizes:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for size, info in MODEL_SIZES.items():\n",
    "        print(f\"{size.upper()}: {info['description']}\")\n",
    "        print(f\"    Model: {info['name']}\")\n",
    "    \n",
    "    print(\"\\nüí° Recommendations:\")\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        if gpu_memory >= 8:\n",
    "            print(\"‚úÖ GPU with 8GB+ memory detected - Can use Medium (M) or Large (L)\")\n",
    "            recommended = 'm'\n",
    "        elif gpu_memory >= 6:\n",
    "            print(\"‚úÖ GPU with 6GB+ memory detected - Recommended: Small (S) or Medium (M)\")\n",
    "            recommended = 's'\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Limited GPU memory - Recommended: Nano (N) or Small (S)\")\n",
    "            recommended = 'n'\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No GPU detected - Recommended: Nano (N) for CPU training\")\n",
    "        recommended = 'n'\n",
    "    \n",
    "    return recommended\n",
    "\n",
    "def initialize_model(model_size='s'):\n",
    "    \"\"\"Initialize YOLO11 pose model\"\"\"\n",
    "    \n",
    "    model_name = MODEL_SIZES[model_size]['name']\n",
    "    print(f\"üöÄ Initializing YOLO11 pose model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load pretrained model\n",
    "        model = YOLO(model_name)\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded successfully!\")\n",
    "        print(f\"üìä Model info:\")\n",
    "        print(f\"   - Architecture: YOLO11{model_size.upper()}\")\n",
    "        print(f\"   - Task: Pose Estimation\")\n",
    "        print(f\"   - Keypoints: 17 (COCO format)\")\n",
    "        print(f\"   - Classes: 1 (person)\")\n",
    "        \n",
    "        # Model summary\n",
    "        model.info(verbose=False)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load model: {e}\")\n",
    "        print(\"üí° Make sure ultralytics is properly installed\")\n",
    "        return None\n",
    "\n",
    "def verify_dataset_config():\n",
    "    \"\"\"Verify dataset configuration is ready\"\"\"\n",
    "    \n",
    "    config_path = data_dir / 'dataset.yaml'\n",
    "    \n",
    "    if not config_path.exists():\n",
    "        print(\"‚ùå Dataset configuration not found!\")\n",
    "        print(\"Please complete the dataset preparation step first.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        print(\"‚úÖ Dataset configuration found:\")\n",
    "        print(f\"   - Path: {config.get('path', 'Not specified')}\")\n",
    "        print(f\"   - Classes: {config.get('nc', 'Not specified')}\")\n",
    "        print(f\"   - Keypoints: {config.get('kpt_shape', 'Not specified')}\")\n",
    "        \n",
    "        # Check if directories exist\n",
    "        base_path = Path(config['path'])\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            split_path = base_path / config.get(split, f\"{split}/images\")\n",
    "            if split_path.exists():\n",
    "                img_count = len(list(split_path.glob('*.*')))\n",
    "                print(f\"   - {split}: {img_count} images\")\n",
    "            else:\n",
    "                print(f\"   - {split}: Directory not found\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading dataset config: {e}\")\n",
    "        return False\n",
    "\n",
    "# Model selection and initialization\n",
    "print(\"üéØ Model Selection and Setup\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get model size recommendation\n",
    "recommended_size = select_model_size()\n",
    "print(f\"\\nüéØ Auto-selected model size: {recommended_size.upper()}\")\n",
    "\n",
    "# You can change this to a different size if needed\n",
    "MODEL_SIZE = recommended_size  # Change to 'n', 's', 'm', 'l', or 'x'\n",
    "\n",
    "# Verify dataset is ready\n",
    "print(f\"\\nüìã Verifying dataset configuration...\")\n",
    "dataset_ready = verify_dataset_config()\n",
    "\n",
    "if dataset_ready:\n",
    "    # Initialize model\n",
    "    print(f\"\\nü§ñ Initializing model...\")\n",
    "    model = initialize_model(MODEL_SIZE)\n",
    "    \n",
    "    if model is not None:\n",
    "        print(f\"\\n‚úÖ Setup Complete!\")\n",
    "        print(f\"üì¶ Model: YOLO11{MODEL_SIZE.upper()}\")\n",
    "        print(f\"üìä Ready for training configuration\")\n",
    "    else:\n",
    "        print(f\"‚ùå Model initialization failed!\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not ready. Please complete previous steps first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7817418",
   "metadata": {},
   "source": [
    "## 8. Configure Training Parameters {#config}\n",
    "\n",
    "Let's configure the training parameters optimized for pose estimation and waste throwing detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7772b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration for Pose Estimation\n",
    "\n",
    "def create_training_config(dataset_size='small'):\n",
    "    \"\"\"Create optimized training configuration based on dataset size\"\"\"\n",
    "    \n",
    "    # Base configuration\n",
    "    config = {\n",
    "        # Training duration\n",
    "        'epochs': 200,\n",
    "        'patience': 30,  # Early stopping patience\n",
    "        \n",
    "        # Data loading\n",
    "        'imgsz': 640,  # Input image size\n",
    "        'batch': 16,   # Batch size (adjust based on GPU memory)\n",
    "        'workers': 4,  # Number of dataloader workers\n",
    "        'cache': True, # Cache images for faster training\n",
    "        \n",
    "        # Optimization\n",
    "        'optimizer': 'AdamW',\n",
    "        'lr0': 0.001,      # Initial learning rate\n",
    "        'lrf': 0.01,       # Final learning rate (lr0 * lrf)\n",
    "        'momentum': 0.937,\n",
    "        'weight_decay': 0.0005,\n",
    "        'warmup_epochs': 3,\n",
    "        'warmup_momentum': 0.8,\n",
    "        'warmup_bias_lr': 0.1,\n",
    "        \n",
    "        # Loss weights (important for pose estimation)\n",
    "        'box': 0.05,    # Box loss weight\n",
    "        'cls': 0.5,     # Classification loss weight  \n",
    "        'kobj': 1.0,    # Keypoint objectness loss weight\n",
    "        'pose': 12.0,   # Pose loss weight (high for pose estimation)\n",
    "        \n",
    "        # Data augmentation\n",
    "        'mosaic': 1.0,      # Mosaic augmentation probability\n",
    "        'mixup': 0.1,       # Mixup augmentation probability\n",
    "        'copy_paste': 0.1,  # Copy-paste augmentation\n",
    "        'degrees': 0.0,     # Rotation range (degrees)\n",
    "        'translate': 0.1,   # Translation fraction\n",
    "        'scale': 0.5,       # Scaling range\n",
    "        'shear': 0.0,       # Shear range\n",
    "        'perspective': 0.0, # Perspective transformation\n",
    "        'flipud': 0.0,      # Vertical flip probability\n",
    "        'fliplr': 0.5,      # Horizontal flip probability\n",
    "        'hsv_h': 0.015,     # Hue augmentation range\n",
    "        'hsv_s': 0.7,       # Saturation augmentation range\n",
    "        'hsv_v': 0.4,       # Value augmentation range\n",
    "        \n",
    "        # Training behavior\n",
    "        'save': True,\n",
    "        'save_period': 10,  # Save checkpoint every N epochs\n",
    "        'cos_lr': True,     # Cosine learning rate scheduler\n",
    "        'close_mosaic': 10, # Disable mosaic in last N epochs\n",
    "        'amp': True,        # Automatic Mixed Precision\n",
    "        'single_cls': False,\n",
    "        'rect': False,      # Rectangular training\n",
    "        'resume': False,\n",
    "        'exist_ok': True,\n",
    "        'pretrained': True,\n",
    "        'verbose': True,\n",
    "        'seed': 42,\n",
    "        'deterministic': True,\n",
    "        'plots': True,\n",
    "        'profile': False,\n",
    "    }\n",
    "    \n",
    "    # Adjust based on dataset size\n",
    "    if dataset_size == 'small':  # < 500 images\n",
    "        config.update({\n",
    "            'epochs': 300,      # More epochs for small datasets\n",
    "            'lr0': 0.002,       # Slightly higher learning rate\n",
    "            'patience': 50,     # More patience\n",
    "            'mosaic': 0.8,      # Reduce mosaic for small datasets\n",
    "            'mixup': 0.15,      # Increase mixup\n",
    "            'copy_paste': 0.15, # Increase copy-paste\n",
    "        })\n",
    "        print(\"üìä Small dataset configuration applied\")\n",
    "        \n",
    "    elif dataset_size == 'medium':  # 500-2000 images\n",
    "        config.update({\n",
    "            'epochs': 200,\n",
    "            'lr0': 0.001,\n",
    "            'patience': 30,\n",
    "        })\n",
    "        print(\"üìä Medium dataset configuration applied\")\n",
    "        \n",
    "    else:  # large: > 2000 images\n",
    "        config.update({\n",
    "            'epochs': 150,      # Fewer epochs for large datasets\n",
    "            'lr0': 0.0008,      # Lower learning rate\n",
    "            'patience': 20,     # Less patience\n",
    "        })\n",
    "        print(\"üìä Large dataset configuration applied\")\n",
    "    \n",
    "    # Adjust batch size based on GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        if gpu_memory >= 12:\n",
    "            config['batch'] = 32\n",
    "            print(\"üöÄ High-end GPU detected - using batch size 32\")\n",
    "        elif gpu_memory >= 8:\n",
    "            config['batch'] = 24\n",
    "            print(\"üöÄ Mid-range GPU detected - using batch size 24\")\n",
    "        elif gpu_memory >= 6:\n",
    "            config['batch'] = 16\n",
    "            print(\"üöÄ Entry-level GPU detected - using batch size 16\")\n",
    "        else:\n",
    "            config['batch'] = 8\n",
    "            print(\"‚ö†Ô∏è  Limited GPU memory - using batch size 8\")\n",
    "    else:\n",
    "        config['batch'] = 4\n",
    "        config['workers'] = 2\n",
    "        print(\"‚ö†Ô∏è  CPU training - using batch size 4\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "def setup_training_environment():\n",
    "    \"\"\"Setup training environment and monitoring\"\"\"\n",
    "    \n",
    "    # Create results directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_name = f\"yolo11{MODEL_SIZE}_pose_{timestamp}\"\n",
    "    experiment_dir = results_dir / experiment_name\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Experiment directory: {experiment_dir}\")\n",
    "    \n",
    "    # Setup Weights & Biases (optional)\n",
    "    use_wandb = True\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.login(anonymous=\"allow\")\n",
    "        print(\"‚úÖ Weights & Biases available for experiment tracking\")\n",
    "    except:\n",
    "        use_wandb = False\n",
    "        print(\"‚ö†Ô∏è  Weights & Biases not available - using local logging only\")\n",
    "    \n",
    "    return experiment_name, use_wandb\n",
    "\n",
    "def estimate_training_time(config, num_samples):\n",
    "    \"\"\"Estimate training time based on configuration\"\"\"\n",
    "    \n",
    "    # Base time estimates (very rough)\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name().lower()\n",
    "        if 'rtx 40' in gpu_name or 'a100' in gpu_name:\n",
    "            time_per_epoch = 0.5  # High-end GPU\n",
    "        elif 'rtx 30' in gpu_name or 'rtx 20' in gpu_name:\n",
    "            time_per_epoch = 1.0  # Mid-range GPU\n",
    "        else:\n",
    "            time_per_epoch = 2.0  # Entry-level GPU\n",
    "    else:\n",
    "        time_per_epoch = 10.0  # CPU training\n",
    "    \n",
    "    # Adjust for dataset size and batch size\n",
    "    time_per_epoch *= (num_samples / 1000) * (16 / config['batch'])\n",
    "    \n",
    "    total_time = time_per_epoch * config['epochs'] / 60  # Convert to minutes\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Estimated training time: {total_time:.1f} minutes ({total_time/60:.1f} hours)\")\n",
    "    \n",
    "    if total_time > 120:  # More than 2 hours\n",
    "        print(\"üí° Consider reducing epochs or using a smaller model for faster training\")\n",
    "\n",
    "# Determine dataset size for configuration\n",
    "config_path = data_dir / 'dataset.yaml'\n",
    "if config_path.exists():\n",
    "    # Count training samples\n",
    "    train_dir = data_dir / 'train' / 'images'\n",
    "    if train_dir.exists():\n",
    "        num_train_samples = len(list(train_dir.glob('*.*')))\n",
    "        \n",
    "        if num_train_samples < 500:\n",
    "            dataset_size = 'small'\n",
    "        elif num_train_samples < 2000:\n",
    "            dataset_size = 'medium'\n",
    "        else:\n",
    "            dataset_size = 'large'\n",
    "        \n",
    "        print(f\"üìä Dataset Analysis:\")\n",
    "        print(f\"   Training samples: {num_train_samples}\")\n",
    "        print(f\"   Dataset size category: {dataset_size}\")\n",
    "        \n",
    "        # Create training configuration\n",
    "        train_config = create_training_config(dataset_size)\n",
    "        \n",
    "        # Setup training environment\n",
    "        experiment_name, use_wandb = setup_training_environment()\n",
    "        \n",
    "        # Estimate training time\n",
    "        estimate_training_time(train_config, num_train_samples)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Training Configuration Summary:\")\n",
    "        print(f\"   Epochs: {train_config['epochs']}\")\n",
    "        print(f\"   Batch size: {train_config['batch']}\")\n",
    "        print(f\"   Learning rate: {train_config['lr0']}\")\n",
    "        print(f\"   Image size: {train_config['imgsz']}\")\n",
    "        print(f\"   Pose loss weight: {train_config['pose']}\")\n",
    "        print(f\"   Augmentation: Mosaic={train_config['mosaic']}, Mixup={train_config['mixup']}\")\n",
    "        \n",
    "        # Save configuration\n",
    "        config_save_path = results_dir / f\"{experiment_name}_config.json\"\n",
    "        with open(config_save_path, 'w') as f:\n",
    "            json.dump(train_config, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Configuration saved: {config_save_path}\")\n",
    "        print(f\"üöÄ Ready to start training!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Training directory not found. Please prepare dataset first.\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset configuration not found. Please prepare dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d984c",
   "metadata": {},
   "source": [
    "## 9. Train the Model {#train}\n",
    "\n",
    "Now we'll execute the training process. This is where the magic happens! The model will learn to detect human poses and specifically recognize throwing motions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Training Process\n",
    "\n",
    "def start_training():\n",
    "    \"\"\"Start the training process with monitoring\"\"\"\n",
    "    \n",
    "    if 'model' not in locals() or model is None:\n",
    "        print(\"‚ùå Model not initialized. Please run the model setup cell first.\")\n",
    "        return None\n",
    "    \n",
    "    if 'train_config' not in locals():\n",
    "        print(\"‚ùå Training configuration not found. Please run the configuration cell first.\")\n",
    "        return None\n",
    "    \n",
    "    config_path = data_dir / 'dataset.yaml'\n",
    "    if not config_path.exists():\n",
    "        print(\"‚ùå Dataset configuration not found. Please prepare the dataset first.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üöÄ Starting YOLO11 Pose Estimation Training...\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üì¶ Model: YOLO11{MODEL_SIZE.upper()}\")\n",
    "    print(f\"üìä Dataset: {config_path}\")\n",
    "    print(f\"‚öôÔ∏è  Configuration: {len(train_config)} parameters\")\n",
    "    print(f\"üéØ Experiment: {experiment_name}\")\n",
    "    \n",
    "    # Start training\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        print(f\"‚è∞ Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Execute training\n",
    "        results = model.train(\n",
    "            data=str(config_path),\n",
    "            project=str(results_dir),\n",
    "            name=experiment_name,\n",
    "            **train_config\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        training_duration = end_time - start_time\n",
    "        \n",
    "        print(\"üéâ Training completed successfully!\")\n",
    "        print(f\"‚è∞ Training duration: {training_duration}\")\n",
    "        print(f\"üìÅ Results saved to: {results_dir / experiment_name}\")\n",
    "        \n",
    "        # Save training summary\n",
    "        training_summary = {\n",
    "            'start_time': start_time.isoformat(),\n",
    "            'end_time': end_time.isoformat(),\n",
    "            'duration': str(training_duration),\n",
    "            'model_size': MODEL_SIZE,\n",
    "            'experiment_name': experiment_name,\n",
    "            'dataset_path': str(config_path),\n",
    "            'configuration': train_config,\n",
    "            'results_path': str(results_dir / experiment_name)\n",
    "        }\n",
    "        \n",
    "        summary_path = results_dir / f\"{experiment_name}_training_summary.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(training_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"üìÑ Training summary saved: {summary_path}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è  Training interrupted by user\")\n",
    "        print(\"üí° You can resume training using the checkpoint in the results directory\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed with error: {e}\")\n",
    "        print(\"üí° Check GPU memory, dataset format, and configuration\")\n",
    "        return None\n",
    "\n",
    "def monitor_training_progress(results_path):\n",
    "    \"\"\"Monitor training progress and display key metrics\"\"\"\n",
    "    \n",
    "    if not results_path.exists():\n",
    "        print(\"‚ùå Training results not found\")\n",
    "        return\n",
    "    \n",
    "    # Look for training log files\n",
    "    log_files = list(results_path.glob(\"*.csv\"))\n",
    "    if not log_files:\n",
    "        print(\"üìä Training logs not available yet\")\n",
    "        return\n",
    "    \n",
    "    # Read training metrics\n",
    "    try:\n",
    "        log_file = log_files[0]  # Usually results.csv\n",
    "        df = pd.read_csv(log_file)\n",
    "        \n",
    "        print(\"üìà Training Progress:\")\n",
    "        print(f\"   Epochs completed: {len(df)}\")\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            latest = df.iloc[-1]\n",
    "            print(f\"   Latest mAP@50: {latest.get('metrics/mAP50(P)', 'N/A'):.4f}\")\n",
    "            print(f\"   Latest mAP@50:95: {latest.get('metrics/mAP50-95(P)', 'N/A'):.4f}\")\n",
    "            print(f\"   Training loss: {latest.get('train/pose_loss', 'N/A')}\")\n",
    "            print(f\"   Validation loss: {latest.get('val/pose_loss', 'N/A')}\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        if len(df) > 5:  # Only plot if we have enough data\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            \n",
    "            # mAP curves\n",
    "            if 'metrics/mAP50(P)' in df.columns:\n",
    "                axes[0, 0].plot(df['epoch'], df['metrics/mAP50(P)'], label='mAP@50')\n",
    "                axes[0, 0].plot(df['epoch'], df['metrics/mAP50-95(P)'], label='mAP@50:95')\n",
    "                axes[0, 0].set_title('Mean Average Precision')\n",
    "                axes[0, 0].set_xlabel('Epoch')\n",
    "                axes[0, 0].set_ylabel('mAP')\n",
    "                axes[0, 0].legend()\n",
    "                axes[0, 0].grid(True)\n",
    "            \n",
    "            # Loss curves\n",
    "            loss_cols = [col for col in df.columns if 'loss' in col and 'train' in col]\n",
    "            if loss_cols:\n",
    "                for col in loss_cols[:3]:  # Plot first 3 loss types\n",
    "                    axes[0, 1].plot(df['epoch'], df[col], label=col.replace('train/', ''))\n",
    "                axes[0, 1].set_title('Training Losses')\n",
    "                axes[0, 1].set_xlabel('Epoch')\n",
    "                axes[0, 1].set_ylabel('Loss')\n",
    "                axes[0, 1].legend()\n",
    "                axes[0, 1].grid(True)\n",
    "            \n",
    "            # Precision and Recall\n",
    "            if 'metrics/precision(P)' in df.columns:\n",
    "                axes[1, 0].plot(df['epoch'], df['metrics/precision(P)'], label='Precision')\n",
    "                axes[1, 0].plot(df['epoch'], df['metrics/recall(P)'], label='Recall')\n",
    "                axes[1, 0].set_title('Precision and Recall')\n",
    "                axes[1, 0].set_xlabel('Epoch')\n",
    "                axes[1, 0].set_ylabel('Score')\n",
    "                axes[1, 0].legend()\n",
    "                axes[1, 0].grid(True)\n",
    "            \n",
    "            # Learning rate\n",
    "            if 'lr/pg0' in df.columns:\n",
    "                axes[1, 1].plot(df['epoch'], df['lr/pg0'])\n",
    "                axes[1, 1].set_title('Learning Rate')\n",
    "                axes[1, 1].set_xlabel('Epoch')\n",
    "                axes[1, 1].set_ylabel('Learning Rate')\n",
    "                axes[1, 1].grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading training logs: {e}\")\n",
    "\n",
    "# Check if everything is ready for training\n",
    "def check_training_readiness():\n",
    "    \"\"\"Check if all prerequisites are met for training\"\"\"\n",
    "    \n",
    "    checks = {\n",
    "        'Model initialized': 'model' in locals() and model is not None,\n",
    "        'Dataset prepared': (data_dir / 'dataset.yaml').exists(),\n",
    "        'Training config ready': 'train_config' in locals(),\n",
    "        'GPU available': torch.cuda.is_available(),\n",
    "        'Training images exist': (data_dir / 'train' / 'images').exists(),\n",
    "        'Validation images exist': (data_dir / 'val' / 'images').exists(),\n",
    "    }\n",
    "    \n",
    "    print(\"üîç Training Readiness Check:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    all_ready = True\n",
    "    for check, status in checks.items():\n",
    "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "        print(f\"{status_icon} {check}\")\n",
    "        if not status:\n",
    "            all_ready = False\n",
    "    \n",
    "    return all_ready\n",
    "\n",
    "# Perform readiness check\n",
    "ready_to_train = check_training_readiness()\n",
    "\n",
    "if ready_to_train:\n",
    "    print(f\"\\nüéØ All systems ready for training!\")\n",
    "    print(f\"\\n‚ö° To start training, run the next cell\")\n",
    "    print(f\"üìä Monitor progress in the output below\")\n",
    "    print(f\"‚è∞ Estimated time: {train_config['epochs']} epochs\")\n",
    "    \n",
    "    # Optionally start training automatically (comment out if you want manual control)\n",
    "    print(f\"\\nüöÄ Starting training in 5 seconds...\")\n",
    "    print(f\"üí° Press Ctrl+C to cancel\")\n",
    "    \n",
    "    import time\n",
    "    try:\n",
    "        for i in range(5, 0, -1):\n",
    "            print(f\"‚è∞ Starting in {i}...\", end='\\r')\n",
    "            time.sleep(1)\n",
    "        print(f\"üöÄ Starting training now!   \")\n",
    "        \n",
    "        # Start training\n",
    "        training_results = start_training()\n",
    "        \n",
    "        if training_results:\n",
    "            print(f\"\\nüìä Training completed! Monitoring results...\")\n",
    "            monitor_training_progress(results_dir / experiment_name)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n‚ö†Ô∏è  Training start cancelled by user\")\n",
    "        print(f\"üí° You can start training manually by calling: start_training()\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\n‚ùå Not ready for training. Please complete the missing steps above.\")\n",
    "    print(f\"üí° Fix the issues marked with ‚ùå and run this cell again.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
